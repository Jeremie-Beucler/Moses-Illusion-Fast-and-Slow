# SE - compute standard errors
std_mean <- function(x) sd(x, na.rm = TRUE)/sqrt(length(x[!is.na(x)]))
# mean SD - return mean of a variable with sd in brackets
mean_sd_function <- function(var, nb_decimals=0) {
mean_var =  round(mean(var, na.rm = TRUE), nb_decimals)
sd_var = round(sd(var, na.rm = TRUE), nb_decimals)
mean_sd = paste(mean_var, " (", sd_var, ")", sep="")
return(mean_sd)
}
# function to obtain the percentage of a given response for a given variable
percent_function <- function(response, var) {
percent = sum(var == response) / length(var) * 100
return(percent)
}
# effect size for Wilcoxon test
rFromWilcox<-function(wilcoxModel, N){
z<- qnorm(wilcoxModel$p.value/2)
r<- z/ sqrt(N)
cat(wilcoxModel$data.name, "Effect Size, r = ", r)
}
# save load function
save_load <- function(object, mode) {
data_folder <- "./Saved_models/"  # Specify the data folder path relative to your working directory
if (mode == "save") {
# Save the object as an .rds file with the object name in the specified folder
file_name <- paste0(data_folder, deparse(substitute(object)), ".rds")
saveRDS(object, file_name)
cat("Object saved as", file_name, "\n")
} else if (mode == "load") {
# Load the object from the .rds file with the object name in the specified folder
file_name <- paste0(data_folder, deparse(substitute(object)), ".rds")
if (file.exists(file_name)) {
object <- readRDS(file_name)
cat("Object loaded from", file_name, "\n")
return(object)
} else {
cat("Error: File", file_name, "does not exist.\n")
}
} else {
cat("Error: Mode must be 'save' or 'load'.\n")
}
}
# load the data from the 3 studies and the pre-test
data <- read.csv("./tidy_data_S1_S2_S3.csv")
# we change some variables types to factor
data <- data %>% mutate(across(c(study, impostor, subject, gender, education, item_number, load_acc, slow, to_exclude, response_stage, resp_cat))) %>%
mutate(conflict = factor(conflict, levels = c("C", "NC"))) %>%
# we create a new confidence variable bounded between 0 and 1
mutate(betaconf = ((conf/100)*(1-0.01) + 0.01/2))
# we compute the direction of change
data$direction = NA
for (row in seq(1, nrow(data), by=2)){
if(data$accuracy[row] == 1 & data$accuracy[row+1] == 1){
data[row, "direction"] = "11"
data[row+1, "direction"] = "11"
} else if(data$accuracy[row] == 0 & data$accuracy[row+1] == 1){
data[row, "direction"] = "01"
data[row+1, "direction"] = "01"
} else if(data$accuracy[row] == 1 & data$accuracy[row+1] == 0){
data[row, "direction"] = "10"
data[row+1, "direction"] = "10"
} else if(data$accuracy[row] == 0 & data$accuracy[row+1] == 0){
data[row, "direction"] = "00"
data[row+1, "direction"] = "00"
}
}
#NA for one response pre-test
data$direction[data$study == "one_resp_pre_test"] <- NA
data$direction <- as.factor(data$direction)
# get number of cores for parallel computations
ncores = availableCores()
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000)
percent_missed_tot <- data %>% filter(response_stage == "a" & study != "one_resp_pre_test") %>%
group_by(study) %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), \(x) round(x, 1)))
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000) as a function of conflict
percent_missed_tot_C <- data %>% filter(response_stage == "a" & study != "one_resp_pre_test") %>%
group_by(study, conflict) %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), round, 1))
# we exclude trials with failed loads or deadlines (as in the paper)
data <- data %>% filter(to_exclude == 0 | is.na(to_exclude))
# here you can comment the lines above to:
# 1) exclude "Don't know" results
# if you want to check the robustness of the confidence results
data = data %>% filter(to_exclude == 0 | is.na(to_exclude)) %>%
filter(resp_cat != 4)
# 2) recode excluded trials as 0 (missed deadline) and keep the responses for
# missed loads (as responses were recorded for those trials) to see whether
# response exclusion drive the results
# data = data %>%
#   mutate(accuracy = if_else(slow == 1, 0, accuracy))
# we compute the mean illusion strength for each item
# initial no-conflict accuracy minus initial conflict accuracy
illusion_strength <- data %>% filter(study == 1 & response_stage == "a") %>%
group_by(item_number, conflict) %>%
summarise(mean_accu = mean(accuracy)*100) %>%
pivot_wider(names_from = "conflict", values_from = "mean_accu") %>%
mutate(illusion_strength = NC - C) %>%
dplyr::select(item_number, illusion_strength)
# compute mean and sd of illusion strength
mean_illu = mean(illusion_strength$illusion_strength)
sd_illu = sd(illusion_strength$illusion_strength)
# we standardize the illusion strength
illusion_strength <- illusion_strength %>%
mutate(illusion_strength = (illusion_strength - mean_illu) / sd_illu)
# we join the tables; select the initial stage data
# we create a variable named group: 0 for no-conflict correct (baseline),
# 1 for conflict correct and 2 for conflict incorrect
temp <- data %>% filter(study == 1 & response_stage == "a") %>%
left_join(illusion_strength) %>% mutate(accuracy = as.factor(accuracy)) %>%
mutate(group = as.factor(case_when(conflict== "NC" & accuracy == 1 ~ 0,
conflict == "C" & accuracy == 1 ~ 1,
conflict == "C" & accuracy == 0 ~ 2,
TRUE ~ 3))) %>% filter(group != 3)
# simple beta regression model
model_illu_S1 = betareg(betaconf ~ group * illusion_strength, data = temp)
# max random structure
formula = betaconf ~ group * illusion_strength + (group*illusion_strength|subject)
# we find the best random structure
model = buildglmmTMB(formula, data = temp, family = beta_family(), buildmerControl=buildmerControl(include = "group * illusion_strength"))
# get the final formula
final_formula = formula(model@model); final_formula
# random structure unsupported or does not improve fit
# we use our first betareg model with robust standard errors
# and clusters per subject and items
model_illu_S1_robust = coeftest(model_illu_S1,
vcov = vcovCL,
type = "HC0",
cluster = ~subject); tidy_df = tidy(model_illu_S1_robust); tidy_df
# effect size
tidy_df = tidy_df %>% mutate(d = round(logoddsratio_to_d(estimate), 2),
d_low = round(logoddsratio_to_d(estimate - (1.96 * std.error)), 2),
d_high = round(logoddsratio_to_d(estimate + (1.96 * std.error)), 2)) %>%
dplyr::select(term, estimate, std.error, d, d_low, d_high);
tidy_df
# we use our first betareg model with robust standard errors
# and clusters per subject and items
model_illu_S1_robust = coeftest(model_illu_S1,
vcov = vcovCL,
type = "HC0",
cluster = ~subject); tidy_df = tidy(model_illu_S1_robust); tidy_df
# effect size
tidy_df = tidy_df %>% mutate(d = round(logoddsratio_to_d(estimate), 2),
d_low = round(logoddsratio_to_d(estimate - (1.96 * std.error)), 2),
d_high = round(logoddsratio_to_d(estimate + (1.96 * std.error)), 2)) %>%
dplyr::select(term, estimate, std.error, d, d_low, d_high);
tidy_df
# we use our first betareg model with robust standard errors
# and clusters per subject and items
model_illu_S1_robust = coeftest(model_illu_S1,
vcov = vcovCL,
type = "HC0",
cluster = ~subject); tidy_df = tidy(model_illu_S1_robust); tidy_df
knitr::opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE)
#load the required libraries
library(tidyverse)
library(ez)
library(multcomp)
library(pastecs)
library(lme4)
library(ggeffects)
library(afex)
library(emmeans)
library(viridis)
library(psych)
library(car)
library(WRS2)
library(glmmTMB)
library(jtools)
library(ggpubr)
library(lmerTest)
library(cowplot)
library(pbkrtest)
library(ggridges)
library(broom.mixed)
library(betareg)
library(lmtest)
library(sandwich)
library(buildmer)
library(parallelly)
library(parameters)
library(effectsize)
# load functions
# SE - compute standard errors
std_mean <- function(x) sd(x, na.rm = TRUE)/sqrt(length(x[!is.na(x)]))
# mean SD - return mean of a variable with sd in brackets
mean_sd_function <- function(var, nb_decimals=0) {
mean_var =  round(mean(var, na.rm = TRUE), nb_decimals)
sd_var = round(sd(var, na.rm = TRUE), nb_decimals)
mean_sd = paste(mean_var, " (", sd_var, ")", sep="")
return(mean_sd)
}
# function to obtain the percentage of a given response for a given variable
percent_function <- function(response, var) {
percent = sum(var == response) / length(var) * 100
return(percent)
}
# effect size for Wilcoxon test
rFromWilcox<-function(wilcoxModel, N){
z<- qnorm(wilcoxModel$p.value/2)
r<- z/ sqrt(N)
cat(wilcoxModel$data.name, "Effect Size, r = ", r)
}
# save load function
save_load <- function(object, mode) {
data_folder <- "./Saved_models/"  # Specify the data folder path relative to your working directory
if (mode == "save") {
# Save the object as an .rds file with the object name in the specified folder
file_name <- paste0(data_folder, deparse(substitute(object)), ".rds")
saveRDS(object, file_name)
cat("Object saved as", file_name, "\n")
} else if (mode == "load") {
# Load the object from the .rds file with the object name in the specified folder
file_name <- paste0(data_folder, deparse(substitute(object)), ".rds")
if (file.exists(file_name)) {
object <- readRDS(file_name)
cat("Object loaded from", file_name, "\n")
return(object)
} else {
cat("Error: File", file_name, "does not exist.\n")
}
} else {
cat("Error: Mode must be 'save' or 'load'.\n")
}
}
# load the data from the 3 studies and the pre-test
data <- read.csv("./tidy_data_S1_S2_S3.csv")
# we change some variables types to factor
data <- data %>% mutate(across(c(study, impostor, subject, gender, education, item_number, load_acc, slow, to_exclude, response_stage, resp_cat))) %>%
mutate(conflict = factor(conflict, levels = c("C", "NC"))) %>%
# we create a new confidence variable bounded between 0 and 1
mutate(betaconf = ((conf/100)*(1-0.01) + 0.01/2))
# we compute the direction of change
data$direction = NA
for (row in seq(1, nrow(data), by=2)){
if(data$accuracy[row] == 1 & data$accuracy[row+1] == 1){
data[row, "direction"] = "11"
data[row+1, "direction"] = "11"
} else if(data$accuracy[row] == 0 & data$accuracy[row+1] == 1){
data[row, "direction"] = "01"
data[row+1, "direction"] = "01"
} else if(data$accuracy[row] == 1 & data$accuracy[row+1] == 0){
data[row, "direction"] = "10"
data[row+1, "direction"] = "10"
} else if(data$accuracy[row] == 0 & data$accuracy[row+1] == 0){
data[row, "direction"] = "00"
data[row+1, "direction"] = "00"
}
}
#NA for one response pre-test
data$direction[data$study == "one_resp_pre_test"] <- NA
data$direction <- as.factor(data$direction)
# get number of cores for parallel computations
ncores = availableCores()
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000)
percent_missed_tot <- data %>% filter(response_stage == "a" & study != "one_resp_pre_test") %>%
group_by(study) %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), \(x) round(x, 1)))
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000) as a function of conflict
percent_missed_tot_C <- data %>% filter(response_stage == "a" & study != "one_resp_pre_test") %>%
group_by(study, conflict) %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), round, 1))
# we exclude trials with failed loads or deadlines (as in the paper)
data <- data %>% filter(to_exclude == 0 | is.na(to_exclude))
# here you can comment the lines above to:
# 1) exclude "Don't know" results
# if you want to check the robustness of the confidence results
data = data %>% filter(to_exclude == 0 | is.na(to_exclude)) %>%
filter(resp_cat != 4)
# 2) recode excluded trials as 0 (missed deadline) and keep the responses for
# missed loads (as responses were recorded for those trials) to see whether
# response exclusion drive the results
# data = data %>%
#   mutate(accuracy = if_else(slow == 1, 0, accuracy))
# we compute the mean illusion strength for each item
# initial no-conflict accuracy minus initial conflict accuracy
illusion_strength <- data %>% filter(study == 1 & response_stage == "a") %>%
group_by(item_number, conflict) %>%
summarise(mean_accu = mean(accuracy)*100) %>%
pivot_wider(names_from = "conflict", values_from = "mean_accu") %>%
mutate(illusion_strength = NC - C) %>%
dplyr::select(item_number, illusion_strength)
# compute mean and sd of illusion strength
mean_illu = mean(illusion_strength$illusion_strength)
sd_illu = sd(illusion_strength$illusion_strength)
# we standardize the illusion strength
illusion_strength <- illusion_strength %>%
mutate(illusion_strength = (illusion_strength - mean_illu) / sd_illu)
# we join the tables; select the initial stage data
# we create a variable named group: 0 for no-conflict correct (baseline),
# 1 for conflict correct and 2 for conflict incorrect
temp <- data %>% filter(study == 1 & response_stage == "a") %>%
left_join(illusion_strength) %>% mutate(accuracy = as.factor(accuracy)) %>%
mutate(group = as.factor(case_when(conflict== "NC" & accuracy == 1 ~ 0,
conflict == "C" & accuracy == 1 ~ 1,
conflict == "C" & accuracy == 0 ~ 2,
TRUE ~ 3))) %>% filter(group != 3)
# simple beta regression model
model_illu_S1 = betareg(betaconf ~ group * illusion_strength, data = temp)
# max random structure
formula = betaconf ~ group * illusion_strength + (group*illusion_strength|subject)
# we find the best random structure
model = buildglmmTMB(formula, data = temp, family = beta_family(), buildmerControl=buildmerControl(include = "group * illusion_strength"))
# get the final formula
final_formula = formula(model@model); final_formula
# we use our first betareg model with robust standard errors
# and clusters per subject and items
model_illu_S1_robust = coeftest(model_illu_S1,
vcov = vcovCL,
type = "HC0",
cluster = ~subject); tidy_df = tidy(model_illu_S1_robust); tidy_df
knitr::opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE)
#load the required libraries
library(tidyverse)
library(ez)
library(multcomp)
library(pastecs)
library(lme4)
library(ggeffects)
library(afex)
library(emmeans)
library(viridis)
library(psych)
library(car)
library(WRS2)
library(glmmTMB)
library(jtools)
library(ggpubr)
library(lmerTest)
library(cowplot)
library(pbkrtest)
library(ggridges)
library(broom.mixed)
library(betareg)
library(lmtest)
library(sandwich)
library(buildmer)
library(parallelly)
library(parameters)
library(effectsize)
# load functions
# SE - compute standard errors
std_mean <- function(x) sd(x, na.rm = TRUE)/sqrt(length(x[!is.na(x)]))
# mean SD - return mean of a variable with sd in brackets
mean_sd_function <- function(var, nb_decimals=0) {
mean_var =  round(mean(var, na.rm = TRUE), nb_decimals)
sd_var = round(sd(var, na.rm = TRUE), nb_decimals)
mean_sd = paste(mean_var, " (", sd_var, ")", sep="")
return(mean_sd)
}
# function to obtain the percentage of a given response for a given variable
percent_function <- function(response, var) {
percent = sum(var == response) / length(var) * 100
return(percent)
}
# effect size for Wilcoxon test
rFromWilcox<-function(wilcoxModel, N){
z<- qnorm(wilcoxModel$p.value/2)
r<- z/ sqrt(N)
cat(wilcoxModel$data.name, "Effect Size, r = ", r)
}
# save load function
save_load <- function(object, mode) {
data_folder <- "./Saved_models/"  # Specify the data folder path relative to your working directory
if (mode == "save") {
# Save the object as an .rds file with the object name in the specified folder
file_name <- paste0(data_folder, deparse(substitute(object)), ".rds")
saveRDS(object, file_name)
cat("Object saved as", file_name, "\n")
} else if (mode == "load") {
# Load the object from the .rds file with the object name in the specified folder
file_name <- paste0(data_folder, deparse(substitute(object)), ".rds")
if (file.exists(file_name)) {
object <- readRDS(file_name)
cat("Object loaded from", file_name, "\n")
return(object)
} else {
cat("Error: File", file_name, "does not exist.\n")
}
} else {
cat("Error: Mode must be 'save' or 'load'.\n")
}
}
# load the data from the 3 studies and the pre-test
data <- read.csv("./tidy_data_S1_S2_S3.csv")
# we change some variables types to factor
data <- data %>% mutate(across(c(study, impostor, subject, gender, education, item_number, load_acc, slow, to_exclude, response_stage, resp_cat))) %>%
mutate(conflict = factor(conflict, levels = c("C", "NC"))) %>%
# we create a new confidence variable bounded between 0 and 1
mutate(betaconf = ((conf/100)*(1-0.01) + 0.01/2))
# we compute the direction of change
data$direction = NA
for (row in seq(1, nrow(data), by=2)){
if(data$accuracy[row] == 1 & data$accuracy[row+1] == 1){
data[row, "direction"] = "11"
data[row+1, "direction"] = "11"
} else if(data$accuracy[row] == 0 & data$accuracy[row+1] == 1){
data[row, "direction"] = "01"
data[row+1, "direction"] = "01"
} else if(data$accuracy[row] == 1 & data$accuracy[row+1] == 0){
data[row, "direction"] = "10"
data[row+1, "direction"] = "10"
} else if(data$accuracy[row] == 0 & data$accuracy[row+1] == 0){
data[row, "direction"] = "00"
data[row+1, "direction"] = "00"
}
}
#NA for one response pre-test
data$direction[data$study == "one_resp_pre_test"] <- NA
data$direction <- as.factor(data$direction)
# get number of cores for parallel computations
ncores = availableCores()
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000)
percent_missed_tot <- data %>% filter(response_stage == "a" & study != "one_resp_pre_test") %>%
group_by(study) %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), \(x) round(x, 1)))
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000) as a function of conflict
percent_missed_tot_C <- data %>% filter(response_stage == "a" & study != "one_resp_pre_test") %>%
group_by(study, conflict) %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), round, 1))
# we exclude trials with failed loads or deadlines (as in the paper)
data <- data %>% filter(to_exclude == 0 | is.na(to_exclude))
# here you can comment the lines above to:
# 1) exclude "Don't know" results
# if you want to check the robustness of the confidence results
#data = data %>% filter(to_exclude == 0 | is.na(to_exclude)) %>%
#   filter(resp_cat != 4)
# 2) recode excluded trials as 0 (missed deadline) and keep the responses for
# missed loads (as responses were recorded for those trials) to see whether
# response exclusion drive the results
# data = data %>%
#   mutate(accuracy = if_else(slow == 1, 0, accuracy))
# we compute the mean illusion strength for each item
# initial no-conflict accuracy minus initial conflict accuracy
illusion_strength <- data %>% filter(study == 1 & response_stage == "a") %>%
group_by(item_number, conflict) %>%
summarise(mean_accu = mean(accuracy)*100) %>%
pivot_wider(names_from = "conflict", values_from = "mean_accu") %>%
mutate(illusion_strength = NC - C) %>%
dplyr::select(item_number, illusion_strength)
# compute mean and sd of illusion strength
mean_illu = mean(illusion_strength$illusion_strength)
sd_illu = sd(illusion_strength$illusion_strength)
# we standardize the illusion strength
illusion_strength <- illusion_strength %>%
mutate(illusion_strength = (illusion_strength - mean_illu) / sd_illu)
# we join the tables; select the initial stage data
# we create a variable named group: 0 for no-conflict correct (baseline),
# 1 for conflict correct and 2 for conflict incorrect
temp <- data %>% filter(study == 1 & response_stage == "a") %>%
left_join(illusion_strength) %>% mutate(accuracy = as.factor(accuracy)) %>%
mutate(group = as.factor(case_when(conflict== "NC" & accuracy == 1 ~ 0,
conflict == "C" & accuracy == 1 ~ 1,
conflict == "C" & accuracy == 0 ~ 2,
TRUE ~ 3))) %>% filter(group != 3)
# simple beta regression model
model_illu_S1 = betareg(betaconf ~ group * illusion_strength, data = temp)
# max random structure
formula = betaconf ~ group * illusion_strength + (group*illusion_strength|subject)
# we find the best random structure
model = buildglmmTMB(formula, data = temp, family = beta_family(), buildmerControl=buildmerControl(include = "group * illusion_strength"))
# get the final formula
final_formula = formula(model@model); final_formula
# random structure unsupported or does not improve fit
# we use our first betareg model with robust standard errors
# and clusters per subject and items
model_illu_S1_robust = coeftest(model_illu_S1,
vcov = vcovCL,
type = "HC0",
cluster = ~subject); tidy_df = tidy(model_illu_S1_robust); tidy_df
# effect size
tidy_df = tidy_df %>% mutate(d = round(logoddsratio_to_d(estimate), 2),
d_low = round(logoddsratio_to_d(estimate - (1.96 * std.error)), 2),
d_high = round(logoddsratio_to_d(estimate + (1.96 * std.error)), 2)) %>%
dplyr::select(term, estimate, std.error, d, d_low, d_high);
tidy_df
# we use our first betareg model with robust standard errors
# and clusters per subject and items
model_illu_S1_robust = coeftest(model_illu_S1,
vcov = vcovCL,
type = "HC0",
cluster = ~subject); tidy_df = tidy(model_illu_S1_robust); tidy_df
