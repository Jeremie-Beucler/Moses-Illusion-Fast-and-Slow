library(tidyverse)
library(ez)
library(multcomp)
library(pastecs)
library(lme4)
library(ggeffects)
library(afex)
library(emmeans)
library(viridis)
library(psych)
library(car)
library(WRS2)
library(glmmTMB)
library(jtools)
library(ggpubr)
library(lmerTest)
library(cowplot)
library(pbkrtest)
library(ggridges)
library(broom.mixed)
library(betareg)
library(lmtest)
library(sandwich)
library(buildmer)
library(parallelly)
library(parameters)
library(effectsize)
# load functions
# SE - compute standard errors
std_mean <- function(x) sd(x, na.rm = TRUE)/sqrt(length(x[!is.na(x)]))
# mean SD - return mean of a variable with sd in brackets
mean_sd_function <- function(var, nb_decimals=0) {
mean_var =  round(mean(var, na.rm = TRUE), nb_decimals)
sd_var = round(sd(var, na.rm = TRUE), nb_decimals)
mean_sd = paste(mean_var, " (", sd_var, ")", sep="")
return(mean_sd)
}
# function to obtain the percentage of a given response for a given variable
percent_function <- function(response, var) {
percent = sum(var == response) / length(var) * 100
return(percent)
}
# effect size for Wilcoxon test
rFromWilcox<-function(wilcoxModel, N){
z<- qnorm(wilcoxModel$p.value/2)
r<- z/ sqrt(N)
cat(wilcoxModel$data.name, "Effect Size, r = ", r)
}
# save load function
save_load <- function(object, mode) {
data_folder <- "./Saved_models/"  # Specify the data folder path relative to your working directory
if (mode == "save") {
# Save the object as an .rds file with the object name in the specified folder
file_name <- paste0(data_folder, deparse(substitute(object)), ".rds")
saveRDS(object, file_name)
cat("Object saved as", file_name, "\n")
} else if (mode == "load") {
# Load the object from the .rds file with the object name in the specified folder
file_name <- paste0(data_folder, deparse(substitute(object)), ".rds")
if (file.exists(file_name)) {
object <- readRDS(file_name)
cat("Object loaded from", file_name, "\n")
return(object)
} else {
cat("Error: File", file_name, "does not exist.\n")
}
} else {
cat("Error: Mode must be 'save' or 'load'.\n")
}
}
# load the data from the 3 studies and the pre-test
data <- read.csv("./tidy_data_S1_S2_S3.csv")
# we change some variables types to factor
data <- data %>% mutate(across(c(study, impostor, subject, gender, education, item_number, load_acc, slow, to_exclude, response_stage, resp_cat))) %>%
mutate(conflict = factor(conflict, levels = c("C", "NC"))) %>%
# we create a new confidence variable bounded between 0 and 1
mutate(betaconf = ((conf/100)*(1-0.01) + 0.01/2))
# we compute the direction of change
data$direction = NA
for (row in seq(1, nrow(data), by=2)){
if(data$accuracy[row] == 1 & data$accuracy[row+1] == 1){
data[row, "direction"] = "11"
data[row+1, "direction"] = "11"
} else if(data$accuracy[row] == 0 & data$accuracy[row+1] == 1){
data[row, "direction"] = "01"
data[row+1, "direction"] = "01"
} else if(data$accuracy[row] == 1 & data$accuracy[row+1] == 0){
data[row, "direction"] = "10"
data[row+1, "direction"] = "10"
} else if(data$accuracy[row] == 0 & data$accuracy[row+1] == 0){
data[row, "direction"] = "00"
data[row+1, "direction"] = "00"
}
}
#NA for one response pre-test
data$direction[data$study == "one_resp_pre_test"] <- NA
data$direction <- as.factor(data$direction)
# get number of cores for parallel computations
ncores = availableCores()
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000)
percent_missed_tot <- data %>% filter(response_stage == "a" & study != "one_resp_pre_test") %>%
group_by(study) %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), \(x) round(x, 1)))
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000) as a function of conflict
percent_missed_tot_C <- data %>% filter(response_stage == "a" & study != "one_resp_pre_test") %>%
group_by(study, conflict) %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), round, 1))
# we exclude trials with failed loads or deadlines (as in the paper)
data <- data %>% filter(to_exclude == 0 | is.na(to_exclude))
# here you can comment the lines above to:
# 1) exclude "Don't know" results
# if you want to check the robustness of the confidence results
#data = data %>% filter(to_exclude == 0 | is.na(to_exclude)) %>%
#   filter(resp_cat != 4)
# 2) recode excluded trials as 0 (missed deadline) and keep the responses for
# missed loads (as responses were recorded for those trials) to see whether
# response exclusion drive the results
# data = data %>%
#   mutate(accuracy = if_else(slow == 1, 0, accuracy))
# we compute the mean illusion strength for each item
# initial no-conflict accuracy minus initial conflict accuracy
illusion_strength <- data %>% filter(study == 1 & response_stage == "a") %>%
group_by(item_number, conflict) %>%
summarise(mean_accu = mean(accuracy)*100) %>%
pivot_wider(names_from = "conflict", values_from = "mean_accu") %>%
mutate(illusion_strength = NC - C) %>%
dplyr::select(item_number, illusion_strength)
# compute mean and sd of illusion strength
mean_illu = mean(illusion_strength$illusion_strength)
sd_illu = sd(illusion_strength$illusion_strength)
# we standardize the illusion strength
illusion_strength <- illusion_strength %>%
mutate(illusion_strength = (illusion_strength - mean_illu) / sd_illu)
# we join the tables; select the initial stage data
# we create a variable named group: 0 for no-conflict correct (baseline),
# 1 for conflict correct and 2 for conflict incorrect
temp <- data %>% filter(study == 1 & response_stage == "a") %>%
left_join(illusion_strength) %>% mutate(accuracy = as.factor(accuracy)) %>%
mutate(group = as.factor(case_when(conflict== "NC" & accuracy == 1 ~ 0,
conflict == "C" & accuracy == 1 ~ 1,
conflict == "C" & accuracy == 0 ~ 2,
TRUE ~ 3))) %>% filter(group != 3)
# simple beta regression model
model_illu_S1 = betareg(betaconf ~ group * illusion_strength, data = temp)
# max random structure
formula = betaconf ~ group * illusion_strength + (group*illusion_strength|subject)
# we find the best random structure
model = buildglmmTMB(formula, data = temp, family = beta_family(), buildmerControl=buildmerControl(include = "group * illusion_strength"))
# get the final formula
final_formula = formula(model@model); final_formula
# random structure unsupported or does not improve fit
# we use our first betareg model with robust standard errors
# and clusters per subject and items
model_illu_S1_robust = coeftest(model_illu_S1,
vcov = vcovCL,
type = "HC0",
cluster = ~subject); tidy_df = tidy(model_illu_S1_robust); tidy_df
# effect size
tidy_df = tidy_df %>% mutate(d = round(logoddsratio_to_d(estimate), 2),
d_low = round(logoddsratio_to_d(estimate - (1.96 * std.error)), 2),
d_high = round(logoddsratio_to_d(estimate + (1.96 * std.error)), 2)) %>%
dplyr::select(term, estimate, std.error, d, d_low, d_high);
tidy_df
# we use our first betareg model with robust standard errors
# and clusters per subject and items
model_illu_S1_robust = coeftest(model_illu_S1,
vcov = vcovCL,
type = "HC0",
cluster = ~subject); tidy_df = tidy(model_illu_S1_robust); tidy_df
knitr::opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE)
#load the required libraries
library(tidyverse)
library(ez)
library(multcomp)
library(pastecs)
library(lme4)
library(ggeffects)
library(afex)
library(emmeans)
library(viridis)
library(psych)
library(car)
library(WRS2)
library(glmmTMB)
library(jtools)
library(ggpubr)
library(lmerTest)
library(cowplot)
library(pbkrtest)
library(ggridges)
library(broom.mixed)
library(betareg)
library(lmtest)
library(sandwich)
library(buildmer)
library(parallelly)
library(parameters)
library(effectsize)
# load functions
# SE - compute standard errors
std_mean <- function(x) sd(x, na.rm = TRUE)/sqrt(length(x[!is.na(x)]))
# mean SD - return mean of a variable with sd in brackets
mean_sd_function <- function(var, nb_decimals=0) {
mean_var =  round(mean(var, na.rm = TRUE), nb_decimals)
sd_var = round(sd(var, na.rm = TRUE), nb_decimals)
mean_sd = paste(mean_var, " (", sd_var, ")", sep="")
return(mean_sd)
}
# function to obtain the percentage of a given response for a given variable
percent_function <- function(response, var) {
percent = sum(var == response) / length(var) * 100
return(percent)
}
# effect size for Wilcoxon test
rFromWilcox<-function(wilcoxModel, N){
z<- qnorm(wilcoxModel$p.value/2)
r<- z/ sqrt(N)
cat(wilcoxModel$data.name, "Effect Size, r = ", r)
}
# save load function
save_load <- function(object, mode) {
data_folder <- "./Saved_models/"  # Specify the data folder path relative to your working directory
if (mode == "save") {
# Save the object as an .rds file with the object name in the specified folder
file_name <- paste0(data_folder, deparse(substitute(object)), ".rds")
saveRDS(object, file_name)
cat("Object saved as", file_name, "\n")
} else if (mode == "load") {
# Load the object from the .rds file with the object name in the specified folder
file_name <- paste0(data_folder, deparse(substitute(object)), ".rds")
if (file.exists(file_name)) {
object <- readRDS(file_name)
cat("Object loaded from", file_name, "\n")
return(object)
} else {
cat("Error: File", file_name, "does not exist.\n")
}
} else {
cat("Error: Mode must be 'save' or 'load'.\n")
}
}
# load the data from the 3 studies and the pre-test
data <- read.csv("./tidy_data_S1_S2_S3.csv")
# we change some variables types to factor
data <- data %>% mutate(across(c(study, impostor, subject, gender, education, item_number, load_acc, slow, to_exclude, response_stage, resp_cat))) %>%
mutate(conflict = factor(conflict, levels = c("C", "NC"))) %>%
# we create a new confidence variable bounded between 0 and 1
mutate(betaconf = ((conf/100)*(1-0.01) + 0.01/2))
# we compute the direction of change
data$direction = NA
for (row in seq(1, nrow(data), by=2)){
if(data$accuracy[row] == 1 & data$accuracy[row+1] == 1){
data[row, "direction"] = "11"
data[row+1, "direction"] = "11"
} else if(data$accuracy[row] == 0 & data$accuracy[row+1] == 1){
data[row, "direction"] = "01"
data[row+1, "direction"] = "01"
} else if(data$accuracy[row] == 1 & data$accuracy[row+1] == 0){
data[row, "direction"] = "10"
data[row+1, "direction"] = "10"
} else if(data$accuracy[row] == 0 & data$accuracy[row+1] == 0){
data[row, "direction"] = "00"
data[row+1, "direction"] = "00"
}
}
#NA for one response pre-test
data$direction[data$study == "one_resp_pre_test"] <- NA
data$direction <- as.factor(data$direction)
# get number of cores for parallel computations
ncores = availableCores()
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000)
percent_missed_tot <- data %>% filter(response_stage == "a" & study != "one_resp_pre_test") %>%
group_by(study) %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), \(x) round(x, 1)))
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000) as a function of conflict
percent_missed_tot_C <- data %>% filter(response_stage == "a" & study != "one_resp_pre_test") %>%
group_by(study, conflict) %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), round, 1))
# we exclude trials with failed loads or deadlines (as in the paper)
data <- data %>% filter(to_exclude == 0 | is.na(to_exclude))
# here you can comment the lines above to:
# 1) exclude "Don't know" results
# if you want to check the robustness of the confidence results
# data = data %>% filter(to_exclude == 0 | is.na(to_exclude)) %>%
#   filter(resp_cat != 4)
# 2) recode excluded trials as 0 (missed deadline) and keep the responses for
# missed loads (as responses were recorded for those trials) to see whether
# response exclusion drive the results
# data = data %>%
#   mutate(accuracy = if_else(slow == 1, 0, accuracy))
temp = data %>% filter((study == 1 & response_stage == "a") | study == "one_resp_pre_test") %>%
filter(conflict == "C" & accuracy == 1) %>%
mutate(study = factor(study, levels = c("one_resp_pre_test", 1)),
log_rt = log(RT))
# simple lm model
model_rt_s1_oneresp = lm(log(RT) ~ study, data = temp)
# # max random structure
formula = log_rt ~ study + (1|subject) + (study|item_number)
# finding optimal random structure
model = buildmer(formula, data = temp, buildmerControl=buildmerControl(include = "study", args=list(control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))))
# looking at the formula
final_formula = formula(model@model); final_formula
# we build the model with the final formula
model_rt_s1_oneresp_1 = lmer(formula = log(RT) ~ 1 + study + (1 | subject) + (1 | item_number),  data = temp, control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
save_load(model_rt_s1_oneresp_1, "save")
# we load the model
model_rt_s1_oneresp_1 = save_load(model_rt_s1_oneresp_1, "load")
# we check that mixed model better than linear model
AIC(model_rt_s1_oneresp, model_rt_s1_oneresp_1)
summ(model_rt_s1_oneresp_1, t.df="k-r", exp=TRUE)
model_parameters(model_rt_s1_oneresp_1, effects = "random")
# estimated means from the model
emm = emmeans(model_rt_s1_oneresp_1, ~ study, type = "response", lmer.df="kenward-roger"); emm
# eff size
eff_size(emm, df.residual(model_rt_s1_oneresp_1), sigma = sigma(model_rt_s1_oneresp_1))
model_parameters(model_rt_s1_oneresp_1, effects = "random")
# estimated means from the model
emm = emmeans(model_rt_s1_oneresp_1, ~ study, type = "response", lmer.df="kenward-roger"); emm
# eff size
eff_size(emm, df.residual(model_rt_s1_oneresp_1), sigma = sigma(model_rt_s1_oneresp_1))
knitr::opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE)
#load the required libraries
library(performance)
library(tidyverse)
library(ez)
library(multcomp)
library(pastecs)
library(lme4)
library(ggeffects)
library(afex)
library(emmeans)
library(viridis)
library(psych)
library(car)
library(WRS2)
library(glmmTMB)
library(jtools)
library(ggpubr)
library(lmerTest)
library(cowplot)
library(pbkrtest)
library(ggridges)
library(broom.mixed)
library(betareg)
library(lmtest)
library(sandwich)
library(buildmer)
library(parallelly)
library(parameters)
library(effectsize)
# load functions
# SE - compute standard errors
std_mean <- function(x) sd(x, na.rm = TRUE)/sqrt(length(x[!is.na(x)]))
# mean SD - return mean of a variable with sd in brackets
mean_sd_function <- function(var, nb_decimals=0) {
mean_var =  round(mean(var, na.rm = TRUE), nb_decimals)
sd_var = round(sd(var, na.rm = TRUE), nb_decimals)
mean_sd = paste(mean_var, " (", sd_var, ")", sep="")
return(mean_sd)
}
# function to obtain the percentage of a given response for a given variable
percent_function <- function(response, var) {
percent = sum(var == response) / length(var) * 100
return(percent)
}
# effect size for Wilcoxon test
rFromWilcox<-function(wilcoxModel, N){
z<- qnorm(wilcoxModel$p.value/2)
r<- z/ sqrt(N)
cat(wilcoxModel$data.name, "Effect Size, r = ", r)
}
# save load function
save_load <- function(object, mode) {
data_folder <- "./Saved_models/"  # Specify the data folder path relative to your working directory
if (mode == "save") {
# Save the object as an .rds file with the object name in the specified folder
file_name <- paste0(data_folder, deparse(substitute(object)), ".rds")
saveRDS(object, file_name)
cat("Object saved as", file_name, "\n")
} else if (mode == "load") {
# Load the object from the .rds file with the object name in the specified folder
file_name <- paste0(data_folder, deparse(substitute(object)), ".rds")
if (file.exists(file_name)) {
object <- readRDS(file_name)
cat("Object loaded from", file_name, "\n")
return(object)
} else {
cat("Error: File", file_name, "does not exist.\n")
}
} else {
cat("Error: Mode must be 'save' or 'load'.\n")
}
}
# load the data from the 3 studies and the pre-test
data <- read.csv("./tidy_data_S4.csv")
# we change some variables types to factor
data <- data %>%
mutate_at(vars(subject, gender, education, item_number, response_block, conflict, resp_cat), as.factor)
# get number of cores for parallel computations
ncores = availableCores()
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000)
temp <- data %>% filter(response_block == "Fast") %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), round, 1))
knitr::kable(temp, col.names = c("% missed (total)", "Raw number / 2000", "% missed load", "% missed deadline"), digits = 1, align = "l", caption = "Missed loads or deadline in Study 4")
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000)
# nb of completed trials by subject
correct_trials <- data %>% filter(response_block == "Fast") %>% group_by(subject) %>%
summarise(nb_trial = sum(slow == 0 & load_acc == 1))
nb_correct_by_subject <- mean_sd_function(correct_trials$nb_trial, 1)
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000)
temp <- data %>% filter(response_block == "Fast") %>%
group_by(conflict) %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), round, 1))
knitr::kable(temp, col.names = c("Conflict", "% missed (total)", "Raw number / 2000", "% missed load", "% missed deadline"), digits = 1, align = "l", caption = "Missed loads or deadline in Study 4 as a function of conflict")
# compute percent of missed deadlines or loads by subject + raw nb of trials (/2000)
temp <- data %>% filter(response_block == "Fast" & conflict == "C") %>%
group_by(impostor) %>%
# exclude missed dealines or missed loads
summarise(percent_missed_tot = sum(slow == 1 |load_acc == 0)/length(slow)*100,
raw_nb_tot = sum(slow == 1 |load_acc == 0),
percent_missed_load = sum(load_acc == 0)/length(slow)*100,
percent_missed_deadline = sum(slow == 1)/length(slow)*100) %>%
mutate(across(where(is.numeric), round, 1))
knitr::kable(temp, col.names = c("Impostor", "% missed (total)", "Raw number / 2000", "% missed load", "% missed deadline"), digits = 1, align = "l", caption = "Missed loads or deadline in Study 4 as a function of impostor strength")
# we remove the trials to exclude in the fast block (slow response and failed load)
# as in the main results of the paper
data = data %>% filter(to_exclude == 0) %>%
# we create a new confidence variable bounded between 0 and 1
mutate(betaconf = ((conf/100)*(1-0.01) + 0.01/2))
# here you can comment the lines above to:
# 1) exclude "Don't know" results
# if you want to check the robustness of the confidence results
# data = data %>% filter(to_exclude == 0) %>%
#   # we create a new confidence variable bounded between 0 and 1
#   mutate(betaconf = ((conf/100)*(1-0.01) + 0.01/2)) %>%
#   filter(resp_cat != 4)
# 2) recode excluded trials as 0 (missed deadline) and keep the responses for
# missed loads (as responses were recorded for those trials) to see whether
# response exclusion drive the results
# data = data %>%
#   # we create a new confidence variable bounded between 0 and 1
#   mutate(betaconf = ((conf/100)*(1-0.01) + 0.01/2),
#          accuracy = case_when(slow == 1 ~ 0,
#                               TRUE ~ accuracy))
temp = data %>% group_by(subject, response_block) %>%
summarise(mean_rt = mean(RT)) %>%
group_by(response_block) %>%
summarise(mean_rt = mean(mean_rt))
knitr::kable(temp, digits=1)
temp = data %>%
mutate(response_block = factor(response_block, levels = c("Fast", "Slow")),
log_rt = log(RT)) %>%
filter(conflict == "C" & accuracy == 1)
# simple linear model
model_S4_rt = lm(log(RT) ~ response_block, data = temp)
# max random structure
formula = log_rt ~ response_block + (response_block|subject) + (response_block|item_number)
# finding the best random structure
model = buildmer(formula, data = temp, buildmerControl=buildmerControl(include = "response_block", args=list(control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))))
# looking at the formula
final_formula = formula(model@model); final_formula
# we build the model with the final formula
model_S4_rt_1 = lmer(formula = log(RT) ~ 1 + response_block + (1 | subject) + (1 | item_number), data = temp, control=lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
save_load(model_S4_rt_1, "save")
# we load the model
model_S4_rt_1 = save_load(model_S4_rt_1, "load")
# we check that mixed model better than linear model
AIC(model_S4_rt, model_S4_rt_1)
# we look at the unadjusted ICC because RE are not of interest here
icc(model_S4_rt_1)$ICC_unadjusted
summ(model_S4_rt_1, t.df="k-r", exp=TRUE)
model_parameters(model_S4_rt_1, effects = "random")
emm_options(lmer.df = "Kenward-Roger")
emm_S4_rt_1 = emmeans(model_S4_rt_1, ~ response_block, type = "response")
save_load(emm_S4_rt_1, "save")
# we load it
emm_S4_rt_1 = save_load(emm_S4_rt_1, "load"); emm_S4_rt_1
# eff size
eff_size(emm_S4_rt_1, df.residual(model_S4_rt_1), sigma = sigma(model_S4_rt_1))
# eff size
eff_size(emm_S4_rt_1, df.residual(model_S4_rt_1), sigma = sigma(model_S4_rt_1))
summ(model_S4_rt_1, t.df="k-r", exp=TRUE)
